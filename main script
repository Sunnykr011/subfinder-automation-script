#!/bin/bash

read -p "Enter Target Domain: " domain
echo -e "\nğŸ” Starting Scan for: \e[1;32m$domain\e[0m\n"

# Function to show real-time progress
progress() {
    pid=$1
    spin='-\|/'
    i=0
    while kill -0 $pid 2>/dev/null; do
        i=$(( (i+1) %4 ))
        echo -ne "\rğŸ”„ Running: ${spin:$i:1}"
        sleep 0.1
    done
    echo -ne "\râœ… Done!           \n"
}

# Function to run commands safely with logging
run_cmd() {
    "$@" 2>>error.log &  # Run in background & log errors
    pid=$!
    progress $pid  # Show progress animation
}

# Step 1: Subdomain Enumeration
echo "ğŸ” Finding Subdomains..."
run_cmd subfinder -d "$domain" -o subs.txt
run_cmd subfinder -dL subs.txt -o sub_subs.txt
wait

# Step 2: Extract only live subdomains
echo "ğŸŒ Checking Live Subdomains..."
run_cmd httpx -silent < sub_subs.txt | tee live_subs.txt
rm subs.txt sub_subs.txt  # Remove old files
wait

# Step 3: Extract URLs from archive sources
echo "ğŸ“‚ Collecting URLs from Archives..."
run_cmd gau < live_subs.txt | tee urls.txt
wait

# Step 4: Crawling for More Hidden Endpoints
echo "ğŸ•· Deep Crawling for Hidden Paths..."
run_cmd gospider -S live_subs.txt -o crawl_output -d 2 -t 10 --js -q
wait

# Final Summary
echo -e "\nğŸš€ \e[1;32mScan Complete!\e[0m"
echo "ğŸ“„ Live Subdomains: live_subs.txt"
echo "ğŸŒ Extracted URLs: urls.txt"

# Display errors if any
if [ -s error.log ]; then
    echo "âš ï¸ \e[1;31mSome errors occurred. Check error.log for details.\e[0m"
else
    rm error.log  # No errors? Clean up
fi
